@article{tibshirani1994lasso,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {267--288},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Shrinkage and Selection via the Lasso},
 urldate = {2024-07-05},
 volume = {58},
 year = {1996}
}
@article{parkcasella2008bayesianlasso,
author = {Trevor Park and George Casella},
title = {The Bayesian Lasso},
journal = {Journal of the American Statistical Association},
volume = {103},
number = {482},
pages = {681--686},
year = {2008},
publisher = {Taylor \& Francis},
doi = {10.1198/016214508000000337},
URL = {https://doi.org/10.1198/016214508000000337},
eprint = {https://doi.org/10.1198/016214508000000337}
}
@online{simpson2021,
  author = {Simpson, Dan and Simpson, Dan},
  title = {The King Must Die (Repost)},
  date = {2021-12-08},
  url = {https://dansblog.netlify.app/2021-12-08-the-king-must-die-repost},
  langid = {en}
}
@Misc{stan2024,
    title = {Stan Modeling Language Users Guide and Reference Manual, Version 2.35},
    author = {{Stan Development Team}},
    year = {2024},
    url = {http://mc-stan.org/},
  }
@article{effron-2004-diabetes,
    ISSN = {00905364},
    URL = {http://www.jstor.org/stable/3448465},
    abstract = {The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a Cp estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.},
    author = {Bradley Efron and Trevor Hastie and Iain Johnstone and Robert Tibshirani},
    journal = {The Annals of Statistics},
    number = {2},
    pages = {407--451},
    publisher = {Institute of Mathematical Statistics},
    title = {Least Angle Regression},
    urldate = {2024-07-07},
    volume = {32},
    year = {2004}
}
